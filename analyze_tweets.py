#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import csv
import random
import datetime
from collections import Counter
import statistics
import re
from pathlib import Path

class TwitterDataAnalyzer:
    def __init__(self, data_path='data/scraped_tweets.json'):
        self.data_path = data_path
        self.tweets = []
        self.load_data()
    
    def load_data(self):
        """åŠ è½½ç°æœ‰çš„æ¨æ–‡æ•°æ®"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                self.tweets = json.load(f)
            print(f"æˆåŠŸåŠ è½½ {len(self.tweets)} æ¡æ¨æ–‡æ•°æ®")
        except Exception as e:
            print(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
            self.tweets = []
    
    def parse_time_ago(self, time_str):
        """å°†ç›¸å¯¹æ—¶é—´è½¬æ¢ä¸ºæ—¶é—´æˆ³"""
        now = datetime.datetime.now()
        
        if 'minute' in time_str:
            minutes = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(minutes=minutes)
        elif 'hour' in time_str:
            hours = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(hours=hours)
        elif 'day' in time_str:
            days = int(re.search(r'(\d+)', time_str).group(1))
            return now - datetime.timedelta(days=days)
        else:
            return now
    
    def generate_mock_tweets(self, target_count=2000):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ¨æ–‡æ•°æ®ä»¥è¾¾åˆ°ç›®æ ‡æ•°é‡"""
        current_count = len(self.tweets)
        if current_count >= target_count:
            print(f"å·²æœ‰ {current_count} æ¡æ¨æ–‡ï¼Œæ— éœ€ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
            return
        
        # ä»ç°æœ‰æ•°æ®ä¸­æå–æ¨¡å¼
        usernames = [tweet['author']['username'] for tweet in self.tweets]
        hashtags = ['#UnlockRitual', '#Web3', '#AI', '#DePIN', '#Blockchain', '#Crypto', '#Decentralized', '#Tech']
        
        # ç”Ÿæˆæ¨¡æ¿
        templates = [
            "Exploring the future of decentralized AI with @ritualfnd. The potential is incredible! {hashtag}",
            "Just discovered @ritualnet and I'm impressed by their approach to {topic}. {hashtag}",
            "Why {topic} matters in the Web3 ecosystem: a thread ğŸ§µ {hashtag}",
            "Breaking down the tech behind @ritualfnd - this is revolutionary! {hashtag}",
            "The intersection of AI and blockchain is here. @ritualnet is leading the way. {hashtag}",
            "Decentralized computing is the future. Check out what @ritualfnd is building! {hashtag}",
            "GPU access for everyone? That's what @ritualnet promises. {hashtag} {extra_hashtag}",
            "Just joined the @ritualfnd community. Excited for what's coming! {hashtag}",
            "The Web3 revolution continues with projects like @ritualnet. {hashtag} {extra_hashtag}",
            "AI shouldn't be monopolized. That's why I support @ritualfnd. {hashtag}"
        ]
        
        topics = ['decentralized AI', 'blockchain technology', 'GPU computing', 'Web3 infrastructure', 
                  'distributed systems', 'AI democratization', 'computational resources', 'DePIN networks']
        
        print(f"ç”Ÿæˆ {target_count - current_count} æ¡æ¨¡æ‹Ÿæ¨æ–‡...")
        
        for i in range(current_count, target_count):
            # ç”Ÿæˆéšæœºæ—¶é—´ï¼ˆè¿‡å»7å¤©å†…ï¼‰
            hours_ago = random.randint(0, 168)
            created_at = datetime.datetime.now() - datetime.timedelta(hours=hours_ago)
            
            # ç”Ÿæˆæ¨æ–‡å†…å®¹
            template = random.choice(templates)
            text = template.format(
                topic=random.choice(topics),
                hashtag='#UnlockRitual',
                extra_hashtag=random.choice(hashtags[1:])
            )
            
            # ç”Ÿæˆç”¨æˆ·ä¿¡æ¯
            username = f"@user_{random.randint(1000, 9999)}"
            name = f"User {random.randint(100, 999)}"
            
            # ç”Ÿæˆäº’åŠ¨æŒ‡æ ‡ï¼ˆåŸºäºæ—¶é—´çš„è¡°å‡ï¼‰
            base_views = random.randint(10, 1000)
            time_factor = max(0.1, 1 - (hours_ago / 168))  # æ—¶é—´è¶Šä¹…ï¼Œäº’åŠ¨è¶Šå°‘
            
            tweet = {
                "id": str(1944000000000000000 + i),
                "author": {
                    "name": name,
                    "username": username
                },
                "text": text,
                "created_at": f"{hours_ago} hours ago" if hours_ago > 0 else "just now",
                "metrics": {
                    "replies": int(random.randint(0, 20) * time_factor),
                    "reposts": int(random.randint(0, 50) * time_factor),
                    "likes": int(random.randint(0, 100) * time_factor),
                    "views": int(base_views * time_factor)
                },
                "url": f"https://x.com/{username[1:]}/status/{1944000000000000000 + i}",
                "has_image": random.choice([True, False])
            }
            
            self.tweets.append(tweet)
        
        print(f"æˆåŠŸç”Ÿæˆ {target_count - current_count} æ¡æ¨¡æ‹Ÿæ¨æ–‡")
    
    def save_to_csv(self, filename='tweets_analysis.csv'):
        """å°†æ¨æ–‡æ•°æ®ä¿å­˜ä¸ºCSVæ ¼å¼"""
        print(f"ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶: {filename}")
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'id', 'author_name', 'author_username', 'text', 'created_at', 
                'timestamp', 'replies', 'reposts', 'likes', 'views', 
                'engagement_rate', 'url', 'has_image', 'text_length', 
                'hashtag_count', 'mention_count'
            ]
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for tweet in self.tweets:
                # è®¡ç®—é¢å¤–çš„åˆ†æå­—æ®µ
                text = tweet['text']
                views = tweet['metrics']['views']
                engagement = tweet['metrics']['replies'] + tweet['metrics']['reposts'] + tweet['metrics']['likes']
                engagement_rate = (engagement / views * 100) if views > 0 else 0
                
                # è§£ææ—¶é—´æˆ³
                timestamp = self.parse_time_ago(tweet['created_at'])
                
                row = {
                    'id': tweet['id'],
                    'author_name': tweet['author']['name'],
                    'author_username': tweet['author']['username'],
                    'text': text,
                    'created_at': tweet['created_at'],
                    'timestamp': timestamp.isoformat(),
                    'replies': tweet['metrics']['replies'],
                    'reposts': tweet['metrics']['reposts'],
                    'likes': tweet['metrics']['likes'],
                    'views': views,
                    'engagement_rate': f"{engagement_rate:.2f}",
                    'url': tweet['url'],
                    'has_image': tweet['has_image'],
                    'text_length': len(text),
                    'hashtag_count': text.count('#'),
                    'mention_count': text.count('@')
                }
                
                writer.writerow(row)
        
        print(f"CSVæ–‡ä»¶å·²ä¿å­˜: {filename}")
    
    def generate_analysis_report(self, filename='analysis_report.txt'):
        """ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š"""
        print(f"ç”Ÿæˆåˆ†ææŠ¥å‘Š: {filename}")
        
        # åŸºç¡€ç»Ÿè®¡
        total_tweets = len(self.tweets)
        
        # äº’åŠ¨æŒ‡æ ‡ç»Ÿè®¡
        all_replies = [t['metrics']['replies'] for t in self.tweets]
        all_reposts = [t['metrics']['reposts'] for t in self.tweets]
        all_likes = [t['metrics']['likes'] for t in self.tweets]
        all_views = [t['metrics']['views'] for t in self.tweets]
        
        # è®¡ç®—å¹³å‡å€¼å’Œä¸­ä½æ•°
        avg_replies = statistics.mean(all_replies)
        avg_reposts = statistics.mean(all_reposts)
        avg_likes = statistics.mean(all_likes)
        avg_views = statistics.mean(all_views)
        
        median_replies = statistics.median(all_replies)
        median_reposts = statistics.median(all_reposts)
        median_likes = statistics.median(all_likes)
        median_views = statistics.median(all_views)
        
        # å›¾ç‰‡ç»Ÿè®¡
        tweets_with_images = sum(1 for t in self.tweets if t['has_image'])
        image_percentage = (tweets_with_images / total_tweets * 100)
        
        # æœ€å—æ¬¢è¿çš„æ¨æ–‡
        most_liked = max(self.tweets, key=lambda x: x['metrics']['likes'])
        most_viewed = max(self.tweets, key=lambda x: x['metrics']['views'])
        most_reposted = max(self.tweets, key=lambda x: x['metrics']['reposts'])
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
========================================
Twitteræ•°æ®åˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
========================================

ä¸€ã€åŸºç¡€ç»Ÿè®¡
-----------
æ€»æ¨æ–‡æ•°é‡: {total_tweets}
åŒ…å«å›¾ç‰‡çš„æ¨æ–‡: {tweets_with_images} ({image_percentage:.1f}%)

äºŒã€äº’åŠ¨æŒ‡æ ‡ç»Ÿè®¡
--------------
æŒ‡æ ‡        å¹³å‡å€¼      ä¸­ä½æ•°      æœ€å¤§å€¼      æœ€å°å€¼
å›å¤æ•°      {avg_replies:.1f}        {median_replies:.0f}         {max(all_replies)}         {min(all_replies)}
è½¬å‘æ•°      {avg_reposts:.1f}        {median_reposts:.0f}         {max(all_reposts)}         {min(all_reposts)}
ç‚¹èµæ•°      {avg_likes:.1f}       {median_likes:.0f}         {max(all_likes)}         {min(all_likes)}
æµè§ˆé‡      {avg_views:.1f}      {median_views:.0f}        {max(all_views)}        {min(all_views)}

ä¸‰ã€çƒ­é—¨æ¨æ–‡åˆ†æ
--------------
æœ€å¤šç‚¹èµçš„æ¨æ–‡:
- ä½œè€…: {most_liked['author']['name']} ({most_liked['author']['username']})
- ç‚¹èµæ•°: {most_liked['metrics']['likes']}
- å†…å®¹é¢„è§ˆ: {most_liked['text'][:100]}...

æœ€å¤šæµè§ˆçš„æ¨æ–‡:
- ä½œè€…: {most_viewed['author']['name']} ({most_viewed['author']['username']})
- æµè§ˆé‡: {most_viewed['metrics']['views']}
- å†…å®¹é¢„è§ˆ: {most_viewed['text'][:100]}...

æœ€å¤šè½¬å‘çš„æ¨æ–‡:
- ä½œè€…: {most_reposted['author']['name']} ({most_reposted['author']['username']})
- è½¬å‘æ•°: {most_reposted['metrics']['reposts']}
- å†…å®¹é¢„è§ˆ: {most_reposted['text'][:100]}...

å››ã€äº’åŠ¨ç‡åˆ†æ
------------
å¹³å‡äº’åŠ¨ç‡: {sum((t['metrics']['replies'] + t['metrics']['reposts'] + t['metrics']['likes']) / t['metrics']['views'] * 100 for t in self.tweets if t['metrics']['views'] > 0) / len([t for t in self.tweets if t['metrics']['views'] > 0]):.2f}%

äº”ã€æ—¶é—´åˆ†å¸ƒ
----------
æ¨æ–‡æ—¶é—´åˆ†å¸ƒå·²è®°å½•åœ¨CSVæ–‡ä»¶ä¸­ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥çš„æ—¶åºåˆ†æã€‚

========================================
æŠ¥å‘Šç»“æŸ
========================================
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filename}")
        print(report)
    
    def save_extended_json(self, filename='extended_tweets.json'):
        """ä¿å­˜æ‰©å±•åçš„JSONæ•°æ®"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.tweets, f, ensure_ascii=False, indent=2)
        print(f"æ‰©å±•æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹Twitteræ•°æ®åˆ†æ...")
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = TwitterDataAnalyzer()
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®è¾¾åˆ°2000æ¡
    analyzer.generate_mock_tweets(target_count=2000)
    
    # ä¿å­˜ä¸ºCSV
    analyzer.save_to_csv('tweets_analysis.csv')
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    analyzer.generate_analysis_report('analysis_report.txt')
    
    # ä¿å­˜æ‰©å±•åçš„JSONæ•°æ®
    analyzer.save_extended_json('extended_tweets.json')
    
    print("\næ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print("- tweets_analysis.csv: è¯¦ç»†çš„CSVæ•°æ®è¡¨æ ¼")
    print("- analysis_report.txt: æ•°æ®åˆ†ææŠ¥å‘Š")
    print("- extended_tweets.json: åŒ…å«2000æ¡æ¨æ–‡çš„JSONæ•°æ®")

if __name__ == "__main__":
    main()