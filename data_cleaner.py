#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitteræ•°æ®æ¸…æ´—å’Œå»é‡å·¥å…·
ç”¨äºå¤„ç†æŠ“å–çš„æ¨æ–‡æ•°æ®ï¼Œå»é™¤é‡å¤å¹¶ç”Ÿæˆå¹²å‡€çš„æ•°æ®é›†
"""

import pandas as pd
import json
import re
from datetime import datetime
import os

class TwitterDataCleaner:
    def __init__(self):
        self.data = None
        self.original_count = 0
        self.cleaned_count = 0
        
    def load_csv(self, filepath):
        """åŠ è½½CSVæ–‡ä»¶"""
        try:
            self.data = pd.read_csv(filepath, encoding='utf-8-sig')
            self.original_count = len(self.data)
            print(f"âœ… æˆåŠŸåŠ è½½ {self.original_count} æ¡æ•°æ®")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½CSVå¤±è´¥: {e}")
            return False
    
    def load_json(self, filepath):
        """åŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
                if 'tweets' in json_data:
                    self.data = pd.DataFrame(json_data['tweets'])
                else:
                    self.data = pd.DataFrame(json_data)
                self.original_count = len(self.data)
                print(f"âœ… æˆåŠŸåŠ è½½ {self.original_count} æ¡æ•°æ®")
                return True
        except Exception as e:
            print(f"âŒ åŠ è½½JSONå¤±è´¥: {e}")
            return False
    
    def remove_duplicates(self):
        """å»é™¤é‡å¤æ¨æ–‡"""
        if self.data is None:
            print("âŒ æ²¡æœ‰åŠ è½½æ•°æ®")
            return
        
        # æ–¹æ³•1ï¼šåŸºäºæ¨æ–‡IDå»é‡
        if 'id' in self.data.columns:
            before = len(self.data)
            self.data = self.data.drop_duplicates(subset=['id'], keep='first')
            after = len(self.data)
            print(f"ğŸ“Œ åŸºäºIDå»é‡: åˆ é™¤äº† {before - after} æ¡é‡å¤")
        
        # æ–¹æ³•2ï¼šåŸºäºæ¨æ–‡å†…å®¹å»é‡ï¼ˆé’ˆå¯¹ç›¸åŒå†…å®¹ä½†ä¸åŒIDçš„æƒ…å†µï¼‰
        if 'text' in self.data.columns or 'æ¨æ–‡å†…å®¹' in self.data.columns:
            text_col = 'text' if 'text' in self.data.columns else 'æ¨æ–‡å†…å®¹'
            before = len(self.data)
            
            # æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
            self.data[text_col] = self.data[text_col].apply(lambda x: ' '.join(str(x).split()) if pd.notna(x) else '')
            
            # åŸºäºæ¸…ç†åçš„æ–‡æœ¬å»é‡
            self.data = self.data.drop_duplicates(subset=[text_col], keep='first')
            after = len(self.data)
            print(f"ğŸ“Œ åŸºäºå†…å®¹å»é‡: åˆ é™¤äº† {before - after} æ¡é‡å¤")
        
        # æ–¹æ³•3ï¼šåŸºäºä½œè€…å’Œæ—¶é—´çš„ç»„åˆå»é‡ï¼ˆåŒä¸€ä½œè€…åŒä¸€æ—¶é—´ä¸å¯èƒ½å‘ä¸¤æ¡æ¨æ–‡ï¼‰
        author_col = None
        time_col = None
        
        if 'author_username' in self.data.columns:
            author_col = 'author_username'
        elif 'ä½œè€…ç”¨æˆ·å' in self.data.columns:
            author_col = 'ä½œè€…ç”¨æˆ·å'
        
        if 'created_at' in self.data.columns:
            time_col = 'created_at'
        elif 'å‘å¸ƒæ—¶é—´' in self.data.columns:
            time_col = 'å‘å¸ƒæ—¶é—´'
        
        if author_col and time_col:
            before = len(self.data)
            self.data = self.data.drop_duplicates(subset=[author_col, time_col], keep='first')
            after = len(self.data)
            print(f"ğŸ“Œ åŸºäºä½œè€…+æ—¶é—´å»é‡: åˆ é™¤äº† {before - after} æ¡é‡å¤")
        
        self.cleaned_count = len(self.data)
        print(f"\nâœ… å»é‡å®Œæˆï¼åŸå§‹æ•°æ®: {self.original_count} æ¡ â†’ æ¸…ç†å: {self.cleaned_count} æ¡")
        print(f"ğŸ“Š æ€»å…±åˆ é™¤: {self.original_count - self.cleaned_count} æ¡é‡å¤æ•°æ®")
    
    def clean_data(self):
        """æ¸…ç†æ•°æ®"""
        if self.data is None:
            print("âŒ æ²¡æœ‰åŠ è½½æ•°æ®")
            return
        
        # åˆ é™¤ç©ºæ¨æ–‡
        text_col = 'text' if 'text' in self.data.columns else 'æ¨æ–‡å†…å®¹'
        if text_col in self.data.columns:
            before = len(self.data)
            self.data = self.data[self.data[text_col].notna()]
            self.data = self.data[self.data[text_col].str.strip() != '']
            after = len(self.data)
            if before - after > 0:
                print(f"ğŸ“Œ åˆ é™¤äº† {before - after} æ¡ç©ºæ¨æ–‡")
        
        # ä¿®å¤æ•°å€¼åˆ—
        numeric_columns = ['replies', 'retweets', 'likes', 'views', 'å›å¤æ•°', 'è½¬å‘æ•°', 'ç‚¹èµæ•°', 'æµè§ˆé‡']
        for col in numeric_columns:
            if col in self.data.columns:
                self.data[col] = pd.to_numeric(self.data[col], errors='coerce').fillna(0).astype(int)
        
        # é‡æ–°è®¡ç®—äº’åŠ¨ç‡
        if all(col in self.data.columns for col in ['replies', 'retweets', 'likes', 'views']):
            self.data['engagement_rate'] = ((self.data['replies'] + self.data['retweets'] + self.data['likes']) / self.data['views'] * 100).round(2)
            self.data.loc[self.data['views'] == 0, 'engagement_rate'] = 0
        
        print("âœ… æ•°æ®æ¸…ç†å®Œæˆ")
    
    def generate_stats(self):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if self.data is None:
            print("âŒ æ²¡æœ‰åŠ è½½æ•°æ®")
            return
        
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»æ¨æ–‡æ•°: {len(self.data)}")
        
        # å”¯ä¸€ä½œè€…æ•°
        author_col = 'author_username' if 'author_username' in self.data.columns else 'ä½œè€…ç”¨æˆ·å'
        if author_col in self.data.columns:
            unique_authors = self.data[author_col].nunique()
            print(f"   å”¯ä¸€ä½œè€…æ•°: {unique_authors}")
            print(f"   å¹³å‡æ¯ä½ä½œè€…å‘æ¨æ•°: {(len(self.data) / unique_authors):.2f}")
        
        # äº’åŠ¨æ•°æ®ç»Ÿè®¡
        if 'likes' in self.data.columns:
            print(f"   å¹³å‡ç‚¹èµæ•°: {self.data['likes'].mean():.2f}")
            print(f"   æœ€é«˜ç‚¹èµæ•°: {self.data['likes'].max()}")
        
        if 'views' in self.data.columns:
            print(f"   å¹³å‡æµè§ˆé‡: {self.data['views'].mean():.2f}")
            print(f"   æœ€é«˜æµè§ˆé‡: {self.data['views'].max()}")
        
        if 'engagement_rate' in self.data.columns:
            print(f"   å¹³å‡äº’åŠ¨ç‡: {self.data['engagement_rate'].mean():.2f}%")
        
        # å›¾ç‰‡ç»Ÿè®¡
        if 'has_image' in self.data.columns:
            image_count = self.data['has_image'].sum()
            print(f"   åŒ…å«å›¾ç‰‡çš„æ¨æ–‡: {image_count} ({image_count/len(self.data)*100:.1f}%)")
        
        # å‘æ¨æœ€å¤šçš„ä½œè€…
        if author_col in self.data.columns:
            print("\nğŸ† å‘æ¨æœ€å¤šçš„å‰10ä½ä½œè€…:")
            top_authors = self.data[author_col].value_counts().head(10)
            for i, (author, count) in enumerate(top_authors.items(), 1):
                print(f"   {i}. {author} - {count} æ¡æ¨æ–‡")
    
    def export_cleaned_data(self, output_dir='cleaned_data'):
        """å¯¼å‡ºæ¸…ç†åçš„æ•°æ®"""
        if self.data is None:
            print("âŒ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å¯¼å‡ºCSV
        csv_path = os.path.join(output_dir, f'cleaned_tweets_{self.cleaned_count}æ¡_{timestamp}.csv')
        self.data.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… å¯¼å‡ºCSV: {csv_path}")
        
        # å¯¼å‡ºJSON
        json_path = os.path.join(output_dir, f'cleaned_tweets_{self.cleaned_count}æ¡_{timestamp}.json')
        
        # å‡†å¤‡JSONæ•°æ®
        json_data = {
            'metadata': {
                'total_tweets': len(self.data),
                'original_count': self.original_count,
                'duplicates_removed': self.original_count - self.cleaned_count,
                'cleaned_at': datetime.now().isoformat(),
                'hashtag': '#UnlockRitual'
            },
            'tweets': self.data.to_dict('records')
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å¯¼å‡ºJSON: {json_path}")
        
        # å¯¼å‡ºExcelï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        excel_path = os.path.join(output_dir, f'cleaned_tweets_{self.cleaned_count}æ¡_{timestamp}.xlsx')
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            self.data.to_excel(writer, sheet_name='æ¨æ–‡æ•°æ®', index=False)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯é¡µ
            stats_data = {
                'ç»Ÿè®¡é¡¹': ['æ€»æ¨æ–‡æ•°', 'åŸå§‹æ•°æ®é‡', 'åˆ é™¤é‡å¤æ•°', 'æ¸…ç†å®Œæˆæ—¶é—´'],
                'æ•°å€¼': [len(self.data), self.original_count, self.original_count - self.cleaned_count, datetime.now().strftime('%Y-%m-%d %H:%M:%S')]
            }
            pd.DataFrame(stats_data).to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
        print(f"âœ… å¯¼å‡ºExcel: {excel_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Twitteræ•°æ®æ¸…æ´—å·¥å…·")
    print("=" * 50)
    
    cleaner = TwitterDataCleaner()
    
    # è¿™é‡Œå¯ä»¥ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
    file_path = input("è¯·è¾“å…¥è¦æ¸…ç†çš„æ–‡ä»¶è·¯å¾„ (CSVæˆ–JSON): ").strip()
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•ååŠ è½½æ•°æ®
    if file_path.endswith('.csv'):
        if not cleaner.load_csv(file_path):
            return
    elif file_path.endswith('.json'):
        if not cleaner.load_json(file_path):
            return
    else:
        print("âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSVæˆ–JSONæ–‡ä»¶")
        return
    
    # æ‰§è¡Œæ¸…ç†æµç¨‹
    print("\nğŸš€ å¼€å§‹æ¸…ç†æ•°æ®...")
    cleaner.remove_duplicates()
    cleaner.clean_data()
    cleaner.generate_stats()
    
    # å¯¼å‡ºæ¸…ç†åçš„æ•°æ®
    print("\nğŸ’¾ å¯¼å‡ºæ¸…ç†åçš„æ•°æ®...")
    cleaner.export_cleaned_data()
    
    print("\nâœ… æ•°æ®æ¸…ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()